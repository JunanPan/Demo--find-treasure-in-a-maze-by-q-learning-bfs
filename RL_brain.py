"""
This part of code is the Q learning brain, which is a brain of the agent.
All decisions are made in here.

"""

import numpy as np
import pandas as pd


class QLearningTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # a list
        self.lr = learning_rate#å­¦ä¹ ç‡
        self.gamma = reward_decay#æŠ˜æ‰£å› å­
        self.epsilon = e_greedy#é‡‡å–è´ªå©ªç­–ç•¥ä¸­çš„æ¦‚ç‡
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)
        #qè¡¨ï¼Œç”¨dataframeç»“æ„å­˜å‚¨
    def choose_action(self, observation):
        '''
        è¾“å…¥ï¼šçº¢è‰²çŸ©å½¢å—å½“å‰ä½ç½®
        è¾“å‡ºï¼šåŠ¨ä½œaction

        '''
        self.check_state_exist(observation)
        #æ£€æµ‹å½“å‰ä½ç½®æ˜¯å¦åœ¨ q_table ä¸­å­˜åœ¨ ï¼ˆcheck_state_existå‡½æ•°ï¼‰
        # action selection

        #æ¥ä¸‹æ¥é‡‡ç”¨ğœ– â€“ è´ªå©ªç­–ç•¥ï¼š
        if np.random.uniform() < self.epsilon:#epsilon = e_greedy = 0.9
            # choose best action
            #ç™¾åˆ†ä¹‹ä¹åçš„æ¦‚ç‡é€‰æ‹©Qè¡¨ä¸­è¾ƒå¤§Qå€¼çš„åŠ¨ä½œ
            state_action = self.q_table.loc[observation, :]
            # some actions may have the same value, randomly choose on in these actions
            action = np.random.choice(state_action[state_action == np.max(state_action)].index)
        else:
            # choose random action
            #ç™¾åˆ†ä¹‹åçš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œ
            action = np.random.choice(self.actions)
        return action

    #Qè¡¨æ›´æ–°å‡½æ•°
    def learn(self, s, a, r, s_):
        '''
        è¾“å…¥ï¼šagentå½“å‰çŠ¶æ€ã€é‡‡å–çš„åŠ¨ä½œã€å¥–åŠ±ã€é‡‡å–åŠ¨ä½œåæ¢ç´¢è€…ä¸‹ä¸€ä¸ªçŠ¶æ€
        è¾“å‡ºï¼šæœ€åå®Œæˆæ›´æ–°çš„Qè¡¨
        '''
        self.check_state_exist(s_)
        #æ£€æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯å¦åœ¨ q_table ä¸­å­˜åœ¨ ï¼ˆcheck_state_existï¼‰

        q_predict = self.q_table.loc[s, a]
        #æ ¹æ®å½“å‰çš„çŠ¶æ€så’Œé‡‡å–çš„åŠ¨ä½œaç»“åˆQè¡¨å¾—å‡ºé¢„æµ‹å€¼q_predict
        if s_ != 'terminal':
            #å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€S_ä¸æ˜¯ç»ˆç‚¹ï¼š
            #åˆ™æŒ‰ç›¸åº”å…¬å¼è®¡ç®—ï¼Œåªè®¡ç®—äº†å…¶ä¸­ä¸€éƒ¨åˆ†
            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  # next state is not terminal
        else:
        #å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€S_æ˜¯ç»ˆç‚¹åˆ™ï¼šq_target = R
            q_target = r  # next state is terminal
        #è®¡ç®—å…¬å¼å‰©ä½™éƒ¨åˆ†ï¼Œä»è€Œå®ŒæˆQè¡¨çš„æ›´æ–°
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update

    def check_state_exist(self, state):
    #æ£€æµ‹å½“å‰ä½ç½®æ˜¯å¦åœ¨ q_table ä¸­å­˜åœ¨ ï¼ˆcheck_state_existå‡½æ•°ï¼‰
        if state not in self.q_table.index:
            # append new state to q table
            self.q_table = self.q_table.append(
                pd.Series(
                    [0]*len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            )